# IBM Hackathon DIA E22 - RAG-Powered Help Center

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![LangChain](https://img.shields.io/badge/LangChain-ğŸ¦œ-green.svg)](https://python.langchain.com/)
[![IBM watsonx.ai](https://img.shields.io/badge/IBM-watsonx.ai-0530ad.svg)](https://www.ibm.com/watsonx)

A Retrieval-Augmented Generation (RAG) system built for the IBM Hackathon, designed to provide intelligent assistance for the PÃ´le LÃ©onard de Vinci using LangChain, Chroma vector database, and IBM watsonx.ai.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Configuration](#configuration)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [How It Works](#how-it-works)
- [Customization](#customization)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [Contact](#contact)

## ğŸ¯ Overview

This project implements a RAG-based question-answering system that leverages:
- **LangChain** for prompt orchestration and RAG workflow
- **Chroma** as a local persistent vector database
- **IBM watsonx.ai** for generative AI capabilities (Llama 3.3 70B)
- **HuggingFace embeddings** (multilingual-e5-large) for semantic search

The system is designed to answer questions about the PÃ´le LÃ©onard de Vinci by retrieving relevant information from a curated knowledge base and generating contextually appropriate responses.

## âœ¨ Features

- ğŸ” **Semantic Search**: Uses HuggingFace embeddings for accurate document retrieval
- ğŸ§  **RAG Pipeline**: Combines retrieval with IBM watsonx.ai for informed responses
- ğŸ’¾ **Persistent Storage**: Chroma vector database stored locally for quick reuse
- ğŸŒ **Multilingual Support**: E5-large embeddings support multiple languages
- ğŸ¨ **Frontend Interface**: React-based help center UI
- ğŸ”„ **Dynamic Updates**: Add new Q&A pairs without rebuilding the entire database

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Query    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Document Loader                â”‚
â”‚  (JSON â†’ LangChain Documents)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      RAG System (Chroma)            â”‚
â”‚  â€¢ HuggingFace Embeddings           â”‚
â”‚  â€¢ MMR Retrieval (k=8)              â”‚
â”‚  â€¢ Persistent Storage               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   IBM watsonx.ai Chat               â”‚
â”‚  â€¢ Llama 3.3 70B Instruct           â”‚
â”‚  â€¢ IAM Authentication               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Response     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Prerequisites

- **Python**: 3.10 or higher
- **Node.js**: For the frontend (npm)
- **IBM Cloud Account**: With access to watsonx.ai
- **Git**: For cloning the repository

## ğŸš€ Installation

### 1. Fork & Clone the Repository

```bash
# Fork the repository on GitHub (top right button)
# Then clone your fork
git clone https://github.com/<your-username>/hackathon_IBM_DIA_E22.git
cd hackathon_IBM_DIA_E22
```

**Important**: Ensure your fork is set to **public** visibility:
- Go to: Settings â†’ Change repository visibility â†’ Public

### 2. Backend Setup

```bash
cd source/backend

# Create a virtual environment
python -m venv .venv

# Activate the virtual environment
# On Windows:
.venv\Scripts\activate
# On macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install langchain langchain-community langchain-chroma chromadb \
            langchain-core ibm-cloud-sdk-core python-dotenv requests
```

### 3. Frontend Setup

```bash
cd source/frontend/help-center

# Install dependencies and start
npm install
npm run dev
```

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file in the root directory with your IBM Cloud credentials:

```env
WATSON_API_KEY=your_watson_api_key_here
PROJECT_ID=your_project_id_here
IBM_URL=https://your-watsonx-endpoint/ml/v1/text/chat?version=...
```

**How to get these credentials:**
1. Log in to [IBM Cloud](https://cloud.ibm.com/)
2. Navigate to your watsonx.ai instance
3. Find your API key and Project ID in the service credentials
4. Copy the appropriate API endpoint URL

### Model Configuration

You can change the generative model in `main.py`:

```python
model_id = "meta-llama/llama-3-3-70b-instruct"  # Change this to use a different model
```

## ğŸ’» Usage

### Running the Backend

```bash
cd source/backend
python main.py
```

**First Run**: The system will create the vector database from `source/database/samples/clean-json-file.json`. This may take a few minutes.

**Subsequent Runs**: The system will load the persisted embeddings, starting much faster.

### Running the Frontend

```bash
cd source/frontend/help-center
npm run dev
```

Then open your browser to the URL shown in the terminal (typically `http://localhost:5173`).

## ğŸ“ Project Structure

```
hackathon_IBM_DIA_E22/
â”œâ”€â”€ source/
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ document_loader.py      # JSON to LangChain Document converter
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ rag_system.py           # Chroma vector DB & retrieval
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ IBMWatsonxChat.py       # IBM watsonx.ai API wrapper
â”‚   â”‚   â”‚   â””â”€â”€ process.py                  # RAG chain assembly
â”‚   â”‚   â””â”€â”€ main.py                         # Entry point & orchestration
â”‚   â”œâ”€â”€ frontend/
â”‚   â”‚   â””â”€â”€ help-center/                    # React frontend
â”‚   â””â”€â”€ database/
â”‚       â”œâ”€â”€ samples/
â”‚       â”‚   â””â”€â”€ clean-json-file.json        # Source FAQ data
â”‚       â””â”€â”€ prod/                           # Persisted Chroma database
â””â”€â”€ .env                                    # Environment variables (not in repo)
```

## ğŸ”§ How It Works

### 1. Document Loading (`document_loader.py`)

Converts the JSON Q&A file into LangChain `Document` objects enriched with metadata:
- Schools
- Topics/Themes
- Users
- Custom attributes

### 2. RAG System (`rag_system.py`)

- **Embeddings**: Uses `intfloat/multilingual-e5-large` from HuggingFace
- **Vector Store**: Chroma database with local persistence (`source/database/prod`)
- **Retrieval**: MMR (Maximal Marginal Relevance) algorithm with k=8 for diverse results
- **Automatic Management**: Creates DB on first run, reuses on subsequent runs

### 3. IBM watsonx Integration (`IBMWatsonxChat.py`)

Custom `BaseChatModel` implementation that:
- Handles IAM authentication with IBM Cloud
- Makes REST API calls to watsonx.ai
- Wraps the Llama 3.3 70B Instruct model

### 4. RAG Pipeline (`process.py`)

Orchestrates the complete flow:
1. Retrieves most relevant documents from Chroma
2. Constructs a prompt with context for the PÃ´le LÃ©onard de Vinci assistant
3. Generates responses using IBM watsonx.ai
4. Returns the final answer

## ğŸ¨ Customization

### Adding New Q&A Pairs

**Method 1: Update the JSON file**
```json
// Edit source/database/samples/clean-json-file.json
{
  "question": "Your new question?",
  "answer": "The answer to provide",
  "metadata": {
    "school": "ESILV",
    "topic": "Admissions"
  }
}
```
Then delete the `source/database/prod` folder and restart to rebuild the database.

**Method 2: Dynamic insertion**
```python
# In your code (see process.py for example)
rag_system.add_question(
    question="New question?",
    answer="New answer",
    metadata={"school": "EMLV", "topic": "Courses"}
)
```

### Adjusting RAG Parameters

In `process.py`, modify retrieval settings:

```python
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 8, "fetch_k": 20}  # Adjust these values
)
```

### Customizing the Prompt

Edit the system prompt in `process.py` to change the assistant's tone or behavior:

```python
system_prompt = """
Your custom prompt here...
Adjust the role, tone, and instructions as needed.
"""
```

## ğŸ› Troubleshooting

### Authentication Errors

**Problem**: `401 Unauthorized` or authentication failures

**Solutions**:
- Verify `WATSON_API_KEY` and `PROJECT_ID` in `.env`
- Check that the IBM_URL endpoint is correct (should end with `/ml/v1/text/chat?version=...`)
- Ensure your IBM Cloud account has access to watsonx.ai
- Try regenerating your API key in IBM Cloud

### Vector Database Issues

**Problem**: Embeddings not loading or database corruption

**Solutions**:
- Delete the `source/database/prod` folder
- Restart the application to rebuild the database
- Check that `clean-json-file.json` is valid JSON

### Dependency Conflicts

**Problem**: Package installation errors

**Solutions**:
```bash
# Create a fresh virtual environment
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows
pip install --upgrade pip
pip install -r requirements.txt  # if available, or install packages individually
```

### Memory Issues

**Problem**: Out of memory when creating embeddings

**Solutions**:
- Process documents in smaller batches
- Use a smaller embedding model
- Increase available RAM or use a machine with more resources

## ğŸ“Š Logs and Debugging

The console output shows detailed information about each step:
- Document loading progress
- Vector database creation/loading
- watsonx.ai API calls
- Retrieved documents and generated responses

Enable verbose logging by setting:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ¤ Contributing

1. Create a feature branch:
   ```bash
   git checkout -b feature/my-awesome-feature
   ```

2. Make your changes and commit:
   ```bash
   git add .
   git commit -m "Add: my awesome feature"
   ```

3. Push to your fork:
   ```bash
   git push origin feature/my-awesome-feature
   ```

4. Create a Pull Request on GitHub

**Requirements**:
- âœ… Keep your fork **public** during the hackathon
- âœ… Follow the existing code structure
- âœ… Add tests for new features (if applicable)
- âœ… Update documentation as needed

## ğŸ“§ Contact

For questions or support during the hackathon:

ğŸ“¬ Email: [kryptosphere@devinci.fr](mailto:kryptosphere@devinci.fr)

## ğŸ† Hackathon Guidelines

- Keep your repository **public** throughout the event
- Follow the provided template structure
- Have fun and learn!

## ğŸ“„ License

This project is part of the IBM Hackathon at PÃ´le LÃ©onard de Vinci.

---

**Good luck during the IBM Hackathon â€” build, learn, and most importantly: have fun!** ğŸš€

Made with â¤ï¸ for the IBM Hackathon DIA E22