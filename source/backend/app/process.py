from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from app.tools.document_loader import load_qa_documents
from app.tools.rag_system import RAGSystem
import os


def format_docs(docs):
    """Formate les documents r√©cup√©r√©s pour le contexte"""
    return "\n\n".join(
        f"Document {i+1} (ID: {doc.metadata.get('id', 'N/A')}):\n{doc.page_content}" 
        for i, doc in enumerate(docs)
    )


def main(
    api_key: str,
    project_id: str,
    url: str,
    model_id: str,
    model_builder,
):
    print("=" * 80)
    print("üöÄ D√©marrage du syst√®me RAG")
    print("=" * 80)
    
    # 1. Charger les documents Q&A
    json_path = "./source/database/samples/clean-json-file.json"
    documents = load_qa_documents(json_path)
    
    # 2. Initialiser le syst√®me RAG avec Chroma
    rag_system = RAGSystem(
        documents=documents,
        persist_directory="./source/database/prod"
    )
    
    # 3. Cr√©er le LLM IBM Watsonx
    print("‚è≥ Initialisation du LLM IBM Watsonx...")
    llm = model_builder(
        api_key=api_key,
        project_id=project_id,
        model_id=model_id,
        api_url=url,
    )
    print("‚úì LLM initialis√©")
    
    # 4. Cr√©er le prompt template pour le RAG
    template = (
        "Tu es un assistant virtuel pour une √©cole. Je vais te donner les questions qui ressemblent le plus √† celle de l'utilisateur et leurs r√©ponses.\n"
        "Si tu ne trouves pas la r√©ponse dans le contexte, dis-le clairement.\n"
        "Soit clair avec l'utilisateur sur ce que tu trouves\n"
        "Voici les questions et r√©ponses de l'√©cole:\n\n"

        "{context}\n\n"

        "R√©ponds uniquement √† la question suivante pos√©e par l'utilisateur en utilisant les r√©ponses de l'√©cole:\n"
        "{question}\n\n"

        "R√©ponse:"
    )
    
    prompt = ChatPromptTemplate.from_template(template)
    
    # 5. Cr√©er la cha√Æne RAG
    rag_chain = (
        {
            "context": rag_system.get_retriever() | format_docs,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    
    print("\n" + "=" * 80)
    print("‚úÖ Syst√®me RAG pr√™t !")
    print("=" * 80 + "\n")
    
    # 6. Exemple de question
    question = "Qu'est-ce qui peut excuser une absence ?"
    
    # G√©n√©ration de la r√©ponse
    print("‚è≥ G√©n√©ration de la r√©ponse...\n")
    response = rag_chain.invoke(question)
    
    print("=" * 80)
    print("üí¨ R√©ponse du syst√®me RAG:")
    print("=" * 80)
    print(response)
    print("=" * 80)
